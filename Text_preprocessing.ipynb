{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('en_disaster.csv', index_col=0, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "not related or not informative         5612\n",
       "other useful information               4332\n",
       "donations and volunteering             2462\n",
       "affected individuals                   1676\n",
       "sympathy and support                   1247\n",
       "infrastructure and utilities damage     994\n",
       "caution and advice                      655\n",
       "Name: InformationType_coarse, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[[not text.isascii() for text in df.TweetText]].InformationType_coarse.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "not related or not informative         20173\n",
       "other useful information               14545\n",
       "donations and volunteering              6463\n",
       "affected individuals                    6333\n",
       "sympathy and support                    3773\n",
       "infrastructure and utilities damage     3565\n",
       "caution and advice                      2516\n",
       "Name: InformationType_coarse, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[[text.isascii() for text in df.TweetText]].InformationType_coarse.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove URL, RT, mention(@)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.ProcessedText = df.TweetText.str.replace(r'http(\\S)+', r'')\n",
    "df.ProcessedText = df.ProcessedText.str.replace(r'http ...', r'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: ProcessedText, dtype: object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.ProcessedText[df.ProcessedText.str.contains(r'http')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.ProcessedText = df.ProcessedText.str.replace(r'(RT|rt)[ ]*@[ ]*[\\S]+',r'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: ProcessedText, dtype: object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.ProcessedText[df.ProcessedText.str.contains(r'RT[ ]?@')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.ProcessedText = df.ProcessedText.str.replace(r'@[\\S]+',r'')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove non-ascii words or characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.ProcessedText = [''.join([i if ord(i) < 128 else '' for i in text]) for text in df.ProcessedText]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.ProcessedText = df.ProcessedText.str.replace(r'_[\\S]?',r'')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove extra space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.ProcessedText = df.ProcessedText.str.replace(r'[ ]{2, }',r' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## &, < and >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.ProcessedText = df.ProcessedText.str.replace(r'&amp;?',r'and')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.ProcessedText = df.ProcessedText.str.replace(r'&lt;',r'<')\n",
    "df.ProcessedText = df.ProcessedText.str.replace(r'&gt;',r'>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert space between words and punctuation marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.ProcessedText = df.ProcessedText.str.replace(r'([\\w\\d]+)([^\\w\\d ]+)', r'\\1 \\2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.ProcessedText = df.ProcessedText.str.replace(r'([^\\w\\d ]+)([\\w\\d]+)', r'\\1 \\2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lowercased and strip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.ProcessedText = df.ProcessedText.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.ProcessedText = df.ProcessedText.str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate text length for later use in LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.ProcessedText_length = [len(text.split(' ')) for text in df.ProcessedText]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14     3867\n",
       "13     3830\n",
       "15     3807\n",
       "18     3736\n",
       "16     3709\n",
       "12     3700\n",
       "17     3658\n",
       "20     3635\n",
       "19     3564\n",
       "21     3555\n",
       "22     3467\n",
       "11     3390\n",
       "10     3183\n",
       "23     3084\n",
       "9      2881\n",
       "24     2631\n",
       "8      2533\n",
       "25     2372\n",
       "26     1979\n",
       "7      1973\n",
       "27     1563\n",
       "6      1547\n",
       "28     1274\n",
       "5      1179\n",
       "29      910\n",
       "4       867\n",
       "30      762\n",
       "31      496\n",
       "32      366\n",
       "33      271\n",
       "34      161\n",
       "35      115\n",
       "36      101\n",
       "37       41\n",
       "38       39\n",
       "42       23\n",
       "39       21\n",
       "41        9\n",
       "40        8\n",
       "43        7\n",
       "45        7\n",
       "46        5\n",
       "50        3\n",
       "51        3\n",
       "52        3\n",
       "44        2\n",
       "48        2\n",
       "53        1\n",
       "56        1\n",
       "55        1\n",
       "60        1\n",
       "47        1\n",
       "101       1\n",
       "63        1\n",
       "Name: ProcessedText_length, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.ProcessedText_length.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop texts with length <=3 and drop duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.ProcessedText_length>3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset=['ProcessedText'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of sample size and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74346"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "not related or not informative         25785\n",
       "other useful information               18877\n",
       "donations and volunteering              8925\n",
       "affected individuals                    8009\n",
       "sympathy and support                    5020\n",
       "infrastructure and utilities damage     4559\n",
       "caution and advice                      3171\n",
       "Name: InformationType_coarse, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.InformationType_coarse.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ProcessedText_BERT'] = '[CLS] '+df.ProcessedText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "df['ProcessedText_BERTbase_length'] = [len(tokenizer.tokenize(sent)) for sent in df.ProcessedText_BERT]\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
    "df['ProcessedText_BERTlarge_length'] = [len(tokenizer.tokenize(sent)) for sent in df.ProcessedText_BERT]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Int label for later use in softmax and cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = dict()\n",
    "for i, l in enumerate(list(df.InformationType_coarse.value_counts().keys())):\n",
    "    label_dict.update({l: i})\n",
    "\n",
    "df['InformationType_label'] = [label_dict[label] for label in df.InformationType_coarse]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('en_disaster.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
